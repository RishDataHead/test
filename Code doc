Here's the comprehensive documentation for the Enterprise Data Loader project �:Enterprise Data Loader - Complete DocumentationTable of ContentsProject OverviewArchitectureFile-by-File DocumentationKey FeaturesUsage GuideConfiguration ReferenceProject OverviewPurposeEnterprise-grade, production-ready data loader for loading CSV/XML files into Oracle databases with:Atomic transactions (all-or-nothing loading)Multithreaded processing for high performanceDetailed logging for troubleshootingError tracking with chunk-level failure detectionAutomatic table creation from file schemaConfigurable delimiters for various file formatsProblem It SolvesLoading large files (millions of rows) into Oracle databases requires:Performance: Parallel processing to handle large volumes quicklyReliability: Atomic rollback ensures data integrity (no partial loads)Observability: Detailed logging to debug failuresFlexibility: Support for different file formats and delimitersArchitecture PatternModular, layered architecture with separation of concerns:Configuration Layer: Manages all settingsUtilities Layer: Logging and validationDatabase Layer: Connection poolingParser Layer: File format handling (CSV/XML)Loader Layer: Core data loading logicApplication Layer: Orchestration and entry pointFile-by-File Documentation1. config.py - Configuration ManagementPurpose: Centralized configuration for the entire applicationWhy It Exists:Separates configuration from code (best practice)Allows easy modification without changing logicSupports environment variables for deployment flexibilityType-safe using Python dataclassesWhat It Does:@dataclass
class DatabaseConfig:
    """Database connection configuration"""
    user: str = field(default_factory=lambda: os.getenv('ORACLE_USER', 'your_username'))
    password: str = field(default_factory=lambda: os.getenv('ORACLE_PASSWORD', 'your_password'))
    dsn: str = field(default_factory=lambda: os.getenv('ORACLE_DSN', 'localhost:1521/XEPDB1'))
    min_pool_conn: int = 2
    max_pool_conn: int = 10Key Components:DatabaseConfigWhy: Keeps database credentials and connection settings in one placeWhat:user, password, dsn: Oracle connection credentialsmin_pool_conn, max_pool_conn: Connection pool size limitsHow: Uses environment variables as defaults for securityLoaderConfigWhy: Controls data loading behavior and performance tuningWhat:staging_table: Target table namechunk_size: Rows per batch (affects memory and performance)max_workers: Parallel thread count (affects CPU usage)batch_errors: Whether to continue on row-level errorscreate_table: Auto-create table from file schemadrop_if_exists: Drop existing table before creationHow: Provides sensible defaults, easily overridableFileConfigWhy: File-specific settings separate from database/loader concernsWhat:input_file: Path to source filefile_encoding: Character encoding (UTF-8, etc.)csv_delimiter: CSV separator characterxml_record_tag: XML element name for recordsxml_schema_file: Optional XSD for validationvalidate_schema: Enable/disable validationHow: Supports multiple file formats with format-specific settingsAppConfigWhy: Top-level configuration containerWhat: Combines all config sections into single objectHow: Provides factory methods (from_dict, from_env) for flexible initializationDesign Decisions:Dataclasses: Automatic __init__, type hints, immutabilityFactory pattern: from_env() and from_dict() for different initialization sourcesEnvironment variable support: Enables 12-factor app deployment2. utils/logger.py - Logging InfrastructurePurpose: Consistent, structured logging across all modulesWhy It Exists:Debugging production issues requires detailed logsMultiple log destinations (file + console) for different use casesSingleton pattern ensures consistent configurationThread-aware logging for parallel execution visibilityWhat It Does:class LoggerFactory:
    """Factory for creating configured loggers"""
    
    _configured = False  # Singleton pattern
    
    @staticmethod
    def setup_logging(log_dir: str = 'logs', log_level: int = logging.INFO):
        """Configure application-wide logging"""Key Components:LoggerFactory.setup_logging()Why: One-time setup ensures all modules use same configurationWhat:Creates logs/ directory if not existsConfigures log file with timestamp: data_loader_20251029_194257.logSets up dual output: file + consoleDefines log format with timestamp, module name, thread name, log levelHow: Uses Python's logging.basicConfig() with custom handlersLoggerFactory.get_logger()Why: Provides logger instances for each moduleWhat: Returns configured logger by nameHow: Auto-calls setup_logging() if not already doneLog Format:2025-10-29 19:43:03,563 - loaders.data_loader - ThreadPoolExecutor-0_0 - INFO - CHUNK 0 - Starting loadTimestamp: When event occurredModule: Which file logged itThread: Which thread (important for parallel processing)Level: INFO, WARNING, ERROR, CRITICALMessage: What happenedDesign Decisions:Singleton: Prevents multiple logging configurationsThread names in logs: Essential for debugging parallel chunk processingFile + console: File for permanent record, console for real-time monitoringTimestamped files: Each run gets unique log file for historical analysis3. utils/validators.py - Data Quality ValidationPurpose: Validate DataFrame quality before database insertionWhy It Exists:Catch data quality issues early (before hitting database)Prevent cryptic Oracle errors with clear validation messagesLog data quality metrics (null counts, duplicates)Sanitize column names for Oracle compatibilityWhat It Does:class DataValidator:
    """Validate DataFrame structure and data quality"""
    
    @staticmethod
    def validate_dataframe(df: pd.DataFrame) -> Tuple[bool, str]:
        """Returns (is_valid, error_message)"""Key Components:validate_dataframe()Why: Prevents loading invalid dataWhat Checks:Empty DataFrame detectionDuplicate column namesNull value counts per columnHow: Returns tuple (True, "") on success or (False, "error message") on failureSide Effect: Logs null statistics for data quality awarenesssanitize_column_names()Why: Oracle has strict naming rules (no spaces, dashes)What: Replaces spaces/dashes with underscores, converts to uppercaseHow: "First Name" → "FIRST_NAME"Design: Consistent with Oracle naming conventionsExample Output:INFO - Null value counts:
INFO -   age: 5 nulls (0.50%)
INFO -   salary: 10 nulls (1.00%)Design Decisions:Static methods: No state needed, pure validation logicTuple return: Both success flag and error message in one callLogging nulls: Helps users understand data quality without failing loadEarly validation: Fails fast before expensive database operations4. database/connection_pool.py - Connection ManagementPurpose: Thread-safe Oracle database connection poolingWhy It Exists:Performance: Reusing connections is faster than creating new onesResource limits: Oracle limits concurrent connectionsThread safety: Multiple threads need safe access to connectionsConnection lifecycle: Proper acquisition and release managementWhat It Does:class OracleConnectionPool:
    """Thread-safe Oracle connection pool"""
    
    def __init__(self, user: str, password: str, dsn: str, 
                 min_conn: int = 2, max_conn: int = 10):
        """Create connection pool with oracledb"""Key Components:init()Why: Initialize pool once, reuse connectionsWhat: Creates pool with min/max connection limitsHow: Uses oracledb.create_pool() with:threaded=True: Allows concurrent accessgetmode=POOL_GETMODE_WAIT: Wait for connection if all busyincrement=1: Add 1 connection at a time when neededget_connection() Context ManagerWhy: Ensures connections are always released (even on errors)What: Acquires connection, yields it, releases on exitHow: Python @contextmanager with try/finallyUsage:with pool.get_connection() as conn:
    cursor = conn.cursor()
    cursor.execute("SELECT ...")
# Connection automatically released hereclose()Why: Cleanup on application shutdownWhat: Closes all pooled connectionsHow: Calls pool.close() from oracledbDesign Decisions:Context manager: Pythonic, prevents connection leaksWait mode: Better than failing when pool exhaustedConfigurable size: Tune for workload (more connections = more parallelism)Thread-safe by design: oracledb pool handles locking internallyConnection Pool Behavior:Initial: 2 connections created
Request 1-2: Use existing connections
Request 3: Create new connection (increment by 1)
Request 11: WAIT (max 10 connections reached)5. parsers/base_parser.py - Parser InterfacePurpose: Abstract base class defining parser contractWhy It Exists:Extensibility: Easy to add new file formats (JSON, Parquet, etc.)Consistency: All parsers have same interfaceType safety: Forces subclasses to implement required methodsDocumentation: Clear contract for parser behaviorWhat It Does:class BaseParser(ABC):
    """Abstract base class for file parsers"""
    
    @abstractmethod
    def parse(self) -> Generator[pd.DataFrame, None, None]:
        """Must yield DataFrame chunks"""
        pass
    
    @abstractmethod
    def validate(self) -> bool:
        """Must validate file format"""
        passKey Components:parse() Abstract MethodWhy: Core functionality - convert file to DataFramesWhat: Must return generator yielding DataFrame chunksHow: Subclasses implement file-format-specific logicGenerator: Memory-efficient for large filesvalidate() Abstract MethodWhy: Fail-fast before processing large filesWhat: Returns True if file is validHow: Subclasses implement format-specific validationDesign Pattern: Template Method PatternBase class defines structureSubclasses fill in format-specific detailsGuarantees all parsers are compatibleWhy Abstract Class:Enforces implementation of required methodsDocuments parser contractEnables polymorphism (loader doesn't care about format)6. parsers/csv_parser.py - CSV File ParserPurpose: Parse CSV files into DataFrame chunks with configurable delimiterWhy It Exists:CSV is most common data exchange formatLarge CSV files don't fit in memoryDifferent CSV dialects (comma, pipe, tab, semicolon)Need streaming/chunked reading for efficiencyWhat It Does:
