"""
Enterprise Data Loader - CSV/XML to Oracle Database
Supports multithreaded loading with schema validation
Single file, ready to use
"""

import oracledb
import pandas as pd
import xml.etree.ElementTree as ET
from lxml import etree
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from threading import Lock, Thread
import time
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import os
import sys
from datetime import datetime
import signal
import json
import hashlib


# ============================================================================
# CONFIGURATION SECTION - MODIFY THESE VALUES
# ============================================================================

CONFIG = {
    # Database Configuration
    'DB_USER': 'your_username',
    'DB_PASSWORD': 'your_password',
    'DB_DSN': 'localhost:1521/XEPDB1',  # host:port/service_name
    
    # Table Configuration
    'STAGING_TABLE': 'STG_DATA_LOAD',
    
    # Performance Configuration
    'CHUNK_SIZE': 10000,      # Rows per chunk
    'MAX_WORKERS': 4,          # Parallel threads
    'MIN_POOL_CONN': 2,
    'MAX_POOL_CONN': 10,
    
    # File Configuration
    'INPUT_FILE': 'data.csv',  # Can be .csv or .xml
    'FILE_ENCODING': 'utf-8',
    
    # XML Configuration (if using XML files)
    'XML_RECORD_TAG': 'record',  # Tag name for each record
    'XML_SCHEMA_FILE': None,      # Path to XSD schema file (optional)
    'VALIDATION_TIMEOUT': 10,     # Schema validation timeout in seconds
    
    # Options
    'CREATE_TABLE': True,
    'DROP_IF_EXISTS': True,
    'BATCH_ERRORS': False,
    'VALIDATE_SCHEMA': True,
    
    # Recovery Options
    'ENABLE_RECOVERY': True,           # Enable checkpoint/recovery
    'CHECKPOINT_FILE': '.loader_checkpoint.json',  # Checkpoint file path
    'RESUME_FROM_CHECKPOINT': True,    # Auto-resume on restart
}


# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)-10s - %(levelname)-8s - %(message)s',
    handlers=[
        logging.FileHandler(f'data_loader_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# CHECKPOINT MANAGER FOR RECOVERY
# ============================================================================

class CheckpointManager:
    """Manages checkpoints for recovery from interruptions"""
    
    def __init__(self, checkpoint_file: str, enabled: bool = True):
        self.checkpoint_file = checkpoint_file
        self.enabled = enabled
        self.lock = Lock()
        
    def _get_file_hash(self, file_path: str) -> str:
        """Get hash of file to detect changes"""
        try:
            with open(file_path, 'rb') as f:
                # Read first and last 1MB for quick hash
                first_chunk = f.read(1024 * 1024)
                f.seek(-min(1024 * 1024, os.path.getsize(file_path)), 2)
                last_chunk = f.read(1024 * 1024)
                file_size = os.path.getsize(file_path)
                
            hash_content = f"{file_size}:{first_chunk}:{last_chunk}"
            return hashlib.md5(hash_content.encode()).hexdigest()
        except Exception as e:
            logger.warning(f"Could not hash file: {e}")
            return "unknown"
    
    def save_checkpoint(self, file_path: str, table_name: str, 
                       completed_chunks: List[int], total_rows: int,
                       total_chunks: int) -> bool:
        """Save checkpoint to file"""
        if not self.enabled:
            return False
            
        try:
            with self.lock:
                checkpoint = {
                    'file_path': file_path,
                    'file_hash': self._get_file_hash(file_path),
                    'table_name': table_name,
                    'completed_chunks': sorted(completed_chunks),
                    'total_rows_loaded': total_rows,
                    'total_chunks': total_chunks,
                    'timestamp': datetime.now().isoformat(),
                    'status': 'in_progress'
                }
                
                with open(self.checkpoint_file, 'w') as f:
                    json.dump(checkpoint, f, indent=2)
                
                logger.debug(f"Checkpoint saved: {len(completed_chunks)}/{total_chunks} chunks")
                return True
                
        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")
            return False
    
    def load_checkpoint(self, file_path: str, table_name: str) -> Optional[Dict[str, Any]]:
        """Load checkpoint if valid"""
        if not self.enabled:
            return None
            
        try:
            if not os.path.exists(self.checkpoint_file):
                logger.info("No checkpoint file found, starting fresh")
                return None
            
            with open(self.checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
            
            # Validate checkpoint
            if checkpoint['file_path'] != file_path:
                logger.warning("Checkpoint file path mismatch, ignoring checkpoint")
                return None
            
            if checkpoint['table_name'] != table_name:
                logger.warning("Checkpoint table name mismatch, ignoring checkpoint")
                return None
            
            current_hash = self._get_file_hash(file_path)
            if checkpoint['file_hash'] != current_hash and current_hash != "unknown":
                logger.warning("Input file has changed, ignoring checkpoint")
                return None
            
            if checkpoint['status'] == 'completed':
                logger.info("Previous load completed successfully")
                return None
            
            logger.info(f"✓ Valid checkpoint found: {len(checkpoint['completed_chunks'])} chunks already loaded")
            logger.info(f"  Resuming from chunk {max(checkpoint['completed_chunks']) + 1 if checkpoint['completed_chunks'] else 0}")
            
            return checkpoint
            
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            return None
    
    def mark_completed(self):
        """Mark load as completed"""
        if not self.enabled:
            return
            
        try:
            if os.path.exists(self.checkpoint_file):
                with open(self.checkpoint_file, 'r') as f:
                    checkpoint = json.load(f)
                
                checkpoint['status'] = 'completed'
                checkpoint['completed_timestamp'] = datetime.now().isoformat()
                
                with open(self.checkpoint_file, 'w') as f:
                    json.dump(checkpoint, f, indent=2)
                
                logger.info("✓ Checkpoint marked as completed")
        except Exception as e:
            logger.error(f"Failed to mark checkpoint completed: {e}")
    
    def clear_checkpoint(self):
        """Remove checkpoint file"""
        try:
            if os.path.exists(self.checkpoint_file):
                os.remove(self.checkpoint_file)
                logger.info("Checkpoint file removed")
        except Exception as e:
            logger.error(f"Failed to remove checkpoint: {e}")


# ============================================================================
# CONNECTION POOL
# ============================================================================

class OracleConnectionPool:
    """Thread-safe Oracle connection pool"""
    
    def __init__(self, user: str, password: str, dsn: str, 
                 min_conn: int = 2, max_conn: int = 10):
        try:
            self.pool = oracledb.create_pool(
                user=user,
                password=password,
                dsn=dsn,
                min=min_conn,
                max=max_conn,
                increment=1,
                threaded=True,
                getmode=oracledb.POOL_GETMODE_WAIT
            )
            logger.info(f"Connection pool created: min={min_conn}, max={max_conn}")
        except Exception as e:
            logger.error(f"Failed to create connection pool: {str(e)}")
            raise
    
    @contextmanager
    def get_connection(self):
        conn = self.pool.acquire()
        try:
            yield conn
        finally:
            self.pool.release(conn)
    
    def close(self):
        self.pool.close()
        logger.info("Connection pool closed")


# ============================================================================
# XML PARSER WITH SCHEMA VALIDATION
# ============================================================================

class XMLParser:
    """Parse and validate XML files"""
    
    def __init__(self, xml_file: str, record_tag: str, schema_file: Optional[str] = None):
        self.xml_file = xml_file
        self.record_tag = record_tag
        self.schema_file = schema_file
        
    def _validate_with_timeout(self, schema, xml_doc, timeout: int) -> tuple:
        """
        Validate XML with timeout using ThreadPoolExecutor
        Returns (success, error_message)
        """
        def validate():
            try:
                is_valid = schema.validate(xml_doc)
                if is_valid:
                    return True, None
                else:
                    errors = []
                    for i, error in enumerate(schema.error_log, 1):
                        errors.append(f"Line {error.line}, Col {error.column}: {error.message}")
                        if i >= 10:  # Limit errors
                            errors.append(f"... and {len(schema.error_log) - 10} more errors")
                            break
                    return False, "\n  ".join(errors)
            except Exception as e:
                return False, str(e)
        
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(validate)
            try:
                result = future.result(timeout=timeout)
                return result
            except TimeoutError:
                return False, f"Validation timed out after {timeout} seconds (possibly incompatible schema)"
            except Exception as e:
                return False, f"Validation error: {str(e)}"
        
    def validate_schema(self, timeout: int = 10) -> bool:
        """Validate XML against XSD schema with timeout"""
        if not self.schema_file:
            logger.warning("No schema file provided, skipping validation")
            return True
        
        if not os.path.exists(self.schema_file):
            logger.error(f"Schema file not found: {self.schema_file}")
            return False
            
        try:
            logger.info(f"Validating XML against schema: {self.schema_file}")
            
            # Parse schema file with error handling
            try:
                logger.info("Parsing XSD schema file...")
                with open(self.schema_file, 'rb') as f:
                    schema_content = f.read()
                schema_root = etree.XML(schema_content)
                schema = etree.XMLSchema(schema_root)
                logger.info("✓ Schema file parsed successfully")
            except etree.XMLSchemaParseError as e:
                logger.error(f"✗ Invalid XSD schema structure: {str(e)}")
                return False
            except etree.XMLSyntaxError as e:
                logger.error(f"✗ Schema file syntax error: {str(e)}")
                return False
            except Exception as e:
                logger.error(f"✗ Error reading schema file: {str(e)}")
                return False
            
            # Parse XML file with error handling
            try:
                logger.info("Parsing XML data file...")
                xml_doc = etree.parse(self.xml_file)
                logger.info("✓ XML file parsed successfully")
            except etree.XMLSyntaxError as e:
                logger.error(f"✗ Invalid XML file syntax: {str(e)}")
                return False
            except Exception as e:
                logger.error(f"✗ Error reading XML file: {str(e)}")
                return False
            
            # Validate with timeout
            logger.info(f"Starting validation (timeout: {timeout}s)...")
            is_valid, error_msg = self._validate_with_timeout(schema, xml_doc, timeout)
            
            if is_valid:
                logger.info("✓ XML schema validation: PASSED")
                return True
            else:
                logger.error("✗ XML schema validation: FAILED")
                if error_msg:
                    logger.error(f"Validation errors:\n  {error_msg}")
                return False
                
        except Exception as e:
            logger.error(f"✗ Unexpected schema validation error: {str(e)}")
            return False
    
    def parse_to_dataframe(self, chunk_size: int = 10000):
        """
        Parse XML file and yield DataFrames in chunks
        Generator function for memory efficiency
        """
        if not os.path.exists(self.xml_file):
            raise FileNotFoundError(f"XML file not found: {self.xml_file}")
            
        try:
            logger.info(f"Parsing XML file: {self.xml_file}")
            logger.info(f"Looking for record tag: '{self.record_tag}'")
            
            # Parse XML iteratively for large files
            context = ET.iterparse(self.xml_file, events=('end',))
            
            records = []
            count = 0
            tags_found = set()
            
            for event, elem in context:
                # Track all tags found
                tags_found.add(elem.tag)
                
                if elem.tag == self.record_tag:
                    # Extract all child elements as dictionary
                    record = {}
                    for child in elem:
                        record[child.tag] = child.text
                    
                    # Only add non-empty records
                    if record:
                        records.append(record)
                        count += 1
                    
                    # Clear element to free memory
                    elem.clear()
                    
                    # Yield chunk when size reached
                    if len(records) >= chunk_size:
                        df = pd.DataFrame(records)
                        yield df
                        records = []
            
            # Yield remaining records
            if records:
                df = pd.DataFrame(records)
                yield df
            
            if count == 0:
                logger.warning(f"No records found with tag '{self.record_tag}'")
                logger.info(f"Tags found in XML: {', '.join(sorted(tags_found))}")
            else:
                logger.info(f"XML parsing complete: {count} records processed")
            
        except ET.ParseError as e:
            logger.error(f"XML parsing error: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Unexpected XML parsing error: {str(e)}")
            raise


# ============================================================================
# DATA LOADER
# ============================================================================

class DataLoader:
    """Enterprise data loader with multithreading support"""
    
    def __init__(self, pool: OracleConnectionPool, table_name: str,
                 chunk_size: int = 10000, max_workers: int = 4,
                 batch_errors: bool = False, checkpoint_manager: Optional[CheckpointManager] = None):
        self.pool = pool
        self.table_name = table_name
        self.chunk_size = chunk_size
        self.max_workers = max_workers
        self.batch_errors = batch_errors
        self.checkpoint_manager = checkpoint_manager
        self.stats_lock = Lock()
        self.total_rows = 0
        self.failed_rows = 0
        self.completed_chunks = set()
        self.interrupted = False
    
    def _validate_dataframe(self, df: pd.DataFrame) -> bool:
        """Validate DataFrame structure and data quality"""
        try:
            # Check for empty DataFrame
            if df.empty:
                logger.warning("DataFrame is empty")
                return False
            
            # Check for duplicate columns
            if df.columns.duplicated().any():
                logger.error("DataFrame has duplicate column names")
                return False
            
            # Log data quality metrics
            null_counts = df.isnull().sum()
            if null_counts.any():
                logger.info("Null value counts:")
                for col, count in null_counts[null_counts > 0].items():
                    logger.info(f"  {col}: {count} nulls ({count/len(df)*100:.2f}%)")
            
            return True
            
        except Exception as e:
            logger.error(f"DataFrame validation error: {str(e)}")
            return False
    
    def _prepare_insert_statement(self, columns: List[str]) -> str:
        """Generate parameterized INSERT statement"""
        cols = ', '.join(columns)
        placeholders = ', '.join([f':{i+1}' for i in range(len(columns))])
        return f"INSERT INTO {self.table_name} ({cols}) VALUES ({placeholders})"
    
    def _load_chunk(self, chunk_data: pd.DataFrame, chunk_id: int) -> Dict[str, Any]:
        """Load a single chunk into Oracle"""
        start_time = time.time()
        rows_inserted = 0
        
        try:
            # Check if interrupted
            if self.interrupted:
                logger.warning(f"Chunk {chunk_id}: Skipped due to interruption")
                return {
                    'chunk_id': chunk_id,
                    'rows_inserted': 0,
                    'success': False,
                    'error': 'Interrupted'
                }
            
            with self.pool.get_connection() as conn:
                cursor = conn.cursor()
                cursor.setinputsizes(None, self.chunk_size)
                
                columns = chunk_data.columns.tolist()
                insert_sql = self._prepare_insert_statement(columns)
                
                # Handle NaN/None values
                chunk_data = chunk_data.where(pd.notnull(chunk_data), None)
                data_tuples = [tuple(row) for row in chunk_data.values]
                
                # Batch insert
                if self.batch_errors:
                    cursor.executemany(insert_sql, data_tuples, batcherrors=True)
                    errors = cursor.getbatcherrors()
                    if errors:
                        logger.warning(f"Chunk {chunk_id}: {len(errors)} row errors")
                        rows_inserted = len(data_tuples) - len(errors)
                    else:
                        rows_inserted = len(data_tuples)
                else:
                    cursor.executemany(insert_sql, data_tuples)
                    rows_inserted = len(data_tuples)
                
                conn.commit()
                cursor.close()
                
            elapsed = time.time() - start_time
            throughput = rows_inserted / elapsed if elapsed > 0 else 0
            
            logger.info(
                f"Chunk {chunk_id}: {rows_inserted} rows in {elapsed:.2f}s "
                f"({throughput:.0f} rows/sec)"
            )
            
            # Mark chunk as completed
            with self.stats_lock:
                self.completed_chunks.add(chunk_id)
            
            return {
                'chunk_id': chunk_id,
                'rows_inserted': rows_inserted,
                'success': True,
                'elapsed': elapsed
            }
            
        except Exception as e:
            logger.error(f"Chunk {chunk_id} failed: {str(e)}")
            return {
                'chunk_id': chunk_id,
                'rows_inserted': 0,
                'success': False,
                'error': str(e)
            }
    
    def _create_staging_table(self, sample_df: pd.DataFrame, drop_if_exists: bool = True):
        """Create staging table from DataFrame schema"""
        try:
            with self.pool.get_connection() as conn:
                cursor = conn.cursor()
                
                if drop_if_exists:
                    try:
                        cursor.execute(f"DROP TABLE {self.table_name} PURGE")
                        logger.info(f"Dropped existing table: {self.table_name}")
                    except oracledb.DatabaseError:
                        pass
                
                type_mapping = {
                    'int64': 'NUMBER',
                    'int32': 'NUMBER',
                    'float64': 'NUMBER',
                    'float32': 'NUMBER',
                    'object': 'VARCHAR2(4000)',
                    'bool': 'NUMBER(1)',
                    'datetime64[ns]': 'TIMESTAMP',
                    'datetime64': 'TIMESTAMP'
                }
                
                columns_def = []
                for col, dtype in sample_df.dtypes.items():
                    clean_col = col.replace(' ', '_').replace('-', '_').upper()
                    oracle_type = type_mapping.get(str(dtype), 'VARCHAR2(4000)')
                    columns_def.append(f"{clean_col} {oracle_type}")
                
                create_sql = f"CREATE TABLE {self.table_name} ({', '.join(columns_def)})"
                cursor.execute(create_sql)
                conn.commit()
                
                logger.info(f"Created table: {self.table_name}")
                logger.info(f"Columns: {', '.join([cd.split()[0] for cd in columns_def])}")
                cursor.close()
                
        except Exception as e:
            logger.error(f"Failed to create table: {str(e)}")
            raise
    
    def load_from_generator(self, data_generator, create_table: bool = True,
                           drop_if_exists: bool = True) -> Dict[str, Any]:
        """Load data from a generator that yields DataFrames"""
        overall_start = time.time()
        
        # Get first chunk
        try:
            first_chunk = next(data_generator)
        except StopIteration:
            logger.error("No data to load")
            return {'success': False, 'error': 'No data found'}
        
        # Validate first chunk
        if not self._validate_dataframe(first_chunk):
            logger.error("Data validation failed")
            return {'success': False, 'error': 'Validation failed'}
        
        # Create table if needed
        if create_table:
            self._create_staging_table(first_chunk, drop_if_exists)
        
        # Process chunks with multithreading
        chunk_id = 0
        futures = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit first chunk
            futures.append(executor.submit(self._load_chunk, first_chunk, chunk_id))
            chunk_id += 1
            
            # Submit remaining chunks
            for chunk in data_generator:
                futures.append(executor.submit(self._load_chunk, chunk, chunk_id))
                chunk_id += 1
            
            # Collect results
            for future in as_completed(futures):
                result = future.result()
                with self.stats_lock:
                    if result['success']:
                        self.total_rows += result['rows_inserted']
                    else:
                        self.failed_rows += result.get('rows_inserted', 0)
        
        overall_elapsed = time.time() - overall_start
        
        return {
            'total_rows': self.total_rows,
            'failed_rows': self.failed_rows,
            'total_chunks': chunk_id,
            'elapsed_seconds': overall_elapsed,
            'rows_per_second': self.total_rows / overall_elapsed if overall_elapsed > 0 else 0,
            'success': self.failed_rows == 0
        }


# ============================================================================
# FILE PROCESSORS
# ============================================================================

def process_csv(file_path: str, chunk_size: int, encoding: str):
    """Generator for CSV chunks"""
    logger.info(f"Processing CSV file: {file_path}")
    return pd.read_csv(file_path, chunksize=chunk_size, 
                      low_memory=False, encoding=encoding)


def process_xml(file_path: str, chunk_size: int, record_tag: str,
                schema_file: Optional[str], validate: bool, timeout: int):
    """Generator for XML chunks"""
    logger.info(f"Processing XML file: {file_path}")
    
    parser = XMLParser(file_path, record_tag, schema_file)
    
    # Validate schema if required
    if validate and schema_file:
        if not parser.validate_schema(timeout=timeout):
            raise ValueError("XML schema validation failed - check logs for details")
    
    return parser.parse_to_dataframe(chunk_size)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def print_statistics(stats: Dict[str, Any]):
    """Print loading statistics"""
    print("\n" + "="*70)
    print(" "*25 + "LOAD STATISTICS")
    print("="*70)
    print(f"{'Total Rows Loaded:':<30} {stats['total_rows']:>20,}")
    print(f"{'Failed Rows:':<30} {stats['failed_rows']:>20,}")
    print(f"{'Total Chunks:':<30} {stats['total_chunks']:>20}")
    
    if stats.get('chunks_skipped', 0) > 0:
        print(f"{'Chunks Processed:':<30} {stats.get('chunks_processed', 0):>20}")
        print(f"{'Chunks Skipped (Resume):':<30} {stats['chunks_skipped']:>20}")
    
    print(f"{'Elapsed Time:':<30} {stats['elapsed_seconds']:>17.2f} sec")
    print(f"{'Throughput:':<30} {stats['rows_per_second']:>15,.0f} rows/sec")
    
    if stats.get('interrupted', False):
        print(f"{'Status:':<30} {'⚠ INTERRUPTED (checkpoint saved)':>20}")
        print("\n➜ Run the script again to resume from where it stopped")
    else:
        print(f"{'Status:':<30} {('✓ SUCCESS' if stats['success'] else '✗ FAILED'):>20}")
    
    print("="*70 + "\n")


def main():
    """Main execution function"""
    
    logger.info("Starting Data Loader")
    logger.info(f"Configuration: {CONFIG['INPUT_FILE']}")
    
    # Override config with environment variables if present
    db_user = os.getenv('ORACLE_USER', CONFIG['DB_USER'])
    db_password = os.getenv('ORACLE_PASSWORD', CONFIG['DB_PASSWORD'])
    db_dsn = os.getenv('ORACLE_DSN', CONFIG['DB_DSN'])
    
    # Determine file type
    file_ext = os.path.splitext(CONFIG['INPUT_FILE'])[1].lower()
    
    if not os.path.exists(CONFIG['INPUT_FILE']):
        logger.error(f"File not found: {CONFIG['INPUT_FILE']}")
        return
    
    # Initialize checkpoint manager
    checkpoint_mgr = None
    if CONFIG['ENABLE_RECOVERY']:
        checkpoint_mgr = CheckpointManager(
            CONFIG['CHECKPOINT_FILE'],
            enabled=CONFIG['ENABLE_RECOVERY']
        )
        
        # Check if we should resume
        if CONFIG['RESUME_FROM_CHECKPOINT']:
            checkpoint = checkpoint_mgr.load_checkpoint(
                CONFIG['INPUT_FILE'],
                CONFIG['STAGING_TABLE']
            )
            if checkpoint:
                logger.info("═" * 70)
                logger.info("  RESUMING FROM PREVIOUS INTERRUPTED LOAD")
                logger.info("═" * 70)
    
    # Create connection pool
    pool = OracleConnectionPool(
        user=db_user,
        password=db_password,
        dsn=db_dsn,
        min_conn=CONFIG['MIN_POOL_CONN'],
        max_conn=CONFIG['MAX_POOL_CONN']
    )
    
    try:
        # Initialize loader with checkpoint manager
        loader = DataLoader(
            pool=pool,
            table_name=CONFIG['STAGING_TABLE'],
            chunk_size=CONFIG['CHUNK_SIZE'],
            max_workers=CONFIG['MAX_WORKERS'],
            batch_errors=CONFIG['BATCH_ERRORS'],
            checkpoint_manager=checkpoint_mgr
        )
        
        # Process file based on type
        if file_ext == '.csv':
            data_gen = process_csv(
                CONFIG['INPUT_FILE'],
                CONFIG['CHUNK_SIZE'],
                CONFIG['FILE_ENCODING']
            )
        elif file_ext == '.xml':
            data_gen = process_xml(
                CONFIG['INPUT_FILE'],
                CONFIG['CHUNK_SIZE'],
                CONFIG['XML_RECORD_TAG'],
                CONFIG['XML_SCHEMA_FILE'],
                CONFIG['VALIDATE_SCHEMA'],
                CONFIG['VALIDATION_TIMEOUT']
            )
        else:
            logger.error(f"Unsupported file type: {file_ext}")
            return
        
        # Load data
        stats = loader.load_from_generator(
            data_gen,
            create_table=CONFIG['CREATE_TABLE'],
            drop_if_exists=CONFIG['DROP_IF_EXISTS'],
            file_path=CONFIG['INPUT_FILE']
        )
        
        # Print results
        print_statistics(stats)
        
        # Clean up checkpoint on successful completion
        if stats['success'] and not stats.get('interrupted', False):
            if checkpoint_mgr:
                checkpoint_mgr.clear_checkpoint()
        
    except Exception as e:
        logger.error(f"Load failed: {str(e)}", exc_info=True)
        raise
    finally:
        pool.close()
        logger.info("Data Loader finished")d: {str(e)}", exc_info=True)
        raise
    finally:
        pool.close()
        logger.info("Data Loader finished")


if __name__ == "__main__":
    main()
