Here's a comprehensive guide on how to run the modular data loader from the main file :Setup and Execution Guide1. Project SetupInstall Dependencies:pip install oracledb pandas lxmlVerify Directory Structure:enterprise_data_loader/
├── __init__.py
├── config.py
├── main.py
├── database/
│   ├── __init__.py
│   └── connection_pool.py
├── parsers/
│   ├── __init__.py
│   ├── base_parser.py
│   ├── csv_parser.py
│   └── xml_parser.py
├── loaders/
│   ├── __init__.py
│   └── data_loader.py
└── utils/
    ├── __init__.py
    ├── logger.py
    └── validators.py2. Running as a Package ModuleFrom Project Root Directory:# Method 1: Run as module
python -m enterprise_data_loader.main

# Method 2: Direct execution
python enterprise_data_loader/main.py3. Configuration MethodsOption A: Modify config.py directly# Edit enterprise_data_loader/config.py
@dataclass
class DatabaseConfig:
    user: str = 'your_username'
    password: str = 'your_password'
    dsn: str = 'localhost:1521/XEPDB1'
    min_pool_conn: int = 2
    max_pool_conn: int = 10

@dataclass
class FileConfig:
    input_file: str = 'data.csv'  # Your CSV/XML file
    file_encoding: str = 'utf-8'Option B: Use Environment Variables# Linux/Mac
export ORACLE_USER="myuser"
export ORACLE_PASSWORD="mypass"
export ORACLE_DSN="localhost:1521/XEPDB1"
python -m enterprise_data_loader.main

# Windows
set ORACLE_USER=myuser
set ORACLE_PASSWORD=mypass
set ORACLE_DSN=localhost:1521/XEPDB1
python -m enterprise_data_loader.mainOption C: Create a Custom Run ScriptCreate run_loader.py in your project root:#!/usr/bin/env python
"""
Custom run script with hardcoded configuration
"""

from enterprise_data_loader.config import AppConfig, DatabaseConfig, LoaderConfig, FileConfig
from enterprise_data_loader.main import DataLoaderApp

def main():
    # Create custom configuration
    config = AppConfig()
    
    # Database settings
    config.database.user = 'myuser'
    config.database.password = 'mypass'
    config.database.dsn = 'localhost:1521/XEPDB1'
    config.database.min_pool_conn = 2
    config.database.max_pool_conn = 10
    
    # Loader settings
    config.loader.staging_table = 'STG_MY_DATA'
    config.loader.chunk_size = 10000
    config.loader.max_workers = 4
    config.loader.batch_errors = False
    config.loader.create_table = True
    config.loader.drop_if_exists = True
    
    # File settings
    config.file.input_file = 'my_data.csv'
    config.file.file_encoding = 'utf-8'
    
    # For XML files, also set:
    # config.file.xml_record_tag = 'record'
    # config.file.xml_schema_file = 'schema.xsd'
    # config.file.validate_schema = True
    
    # Run the loader
    app = DataLoaderApp(config)
    success = app.run()
    
    if success:
        print("Data loading completed successfully!")
    else:
        print("Data loading failed. Check logs for details.")

if __name__ == "__main__":
    main()Execute Custom Script:python run_loader.py4. Advanced Usage: Programmatic ControlCreate custom_load.py for fine-grained control:"""
Advanced usage with manual component orchestration
"""

from enterprise_data_loader import (
    OracleConnectionPool,
    DataLoader,
    CSVParser,
    XMLParser
)
from enterprise_data_loader.utils.logger import LoggerFactory

# Setup logging
LoggerFactory.setup_logging(log_dir='logs', log_level=20)  # 20 = INFO
logger = LoggerFactory.get_logger(__name__)

def load_csv_data():
    """Load CSV data with custom settings"""
    
    # Initialize connection pool
    pool = OracleConnectionPool(
        user='myuser',
        password='mypass',
        dsn='localhost:1521/XEPDB1',
        min_conn=2,
        max_conn=10
    )
    
    try:
        # Create CSV parser
        parser = CSVParser(
            file_path='large_file.csv',
            chunk_size=50000,  # Larger chunks
            encoding='utf-8'
        )
        
        # Validate file
        if not parser.validate():
            logger.error("CSV validation failed")
            return False
        
        # Initialize loader
        loader = DataLoader(
            pool=pool,
            table_name='MY_STAGING_TABLE',
            chunk_size=50000,
            max_workers=8,  # More parallel threads
            batch_errors=True  # Continue on row errors
        )
        
        # Execute load
        stats = loader.load_from_generator(
            data_generator=parser.parse(),
            create_table=True,
            drop_if_exists=True
        )
        
        # Print results
        print(f"
Loaded {stats['total_rows']:,} rows")
        print(f"Time: {stats['elapsed_seconds']:.2f} seconds")
        print(f"Throughput: {stats['rows_per_second']:,.0f} rows/sec")
        
        return stats['success']
        
    finally:
        pool.close()

def load_xml_data():
    """Load XML data with schema validation"""
    
    pool = OracleConnectionPool(
        user='myuser',
        password='mypass',
        dsn='localhost:1521/XEPDB1'
    )
    
    try:
        # Create XML parser with schema
        parser = XMLParser(
            file_path='data.xml',
            record_tag='record',
            chunk_size=10000,
            schema_file='schema.xsd'  # Optional
        )
        
        # Validate XML schema
        if not parser.validate():
            logger.error("XML schema validation failed")
            return False
        
        # Initialize loader
        loader = DataLoader(
            pool=pool,
            table_name='XML_STAGING_TABLE',
            chunk_size=10000,
            max_workers=4
        )
        
        # Execute load
        stats = loader.load_from_generator(
            data_generator=parser.parse(),
            create_table=True
        )
        
        print(f"
Loaded {stats['total_rows']:,} records from XML")
        
        return stats['success']
        
    finally:
        pool.close()

if __name__ == "__main__":
    # Choose which loader to run
    load_csv_data()
    # load_xml_data()Execute:python custom_load.py5. Command-Line Arguments (Enhanced Version)Modify main.py to accept CLI arguments:Add this to the top of main.py:import argparse

def parse_arguments():
    """Parse command-line arguments"""
    parser = argparse.ArgumentParser(
        description='Enterprise Data Loader - CSV/XML to Oracle'
    )
    
    parser.add_argument(
        '-f', '--file',
        required=True,
        help='Input file path (CSV or XML)'
    )
    
    parser.add_argument(
        '-t', '--table',
        required=True,
        help='Target table name'
    )
    
    parser.add_argument(
        '-u', '--user',
        default=os.getenv('ORACLE_USER'),
        help='Oracle username'
    )
    
    parser.add_argument(
        '-p', '--password',
        default=os.getenv('ORACLE_PASSWORD'),
        help='Oracle password'
    )
    
    parser.add_argument(
        '-d', '--dsn',
        default='localhost:1521/XEPDB1',
        help='Oracle DSN (host:port/service)'
    )
    
    parser.add_argument(
        '-c', '--chunk-size',
        type=int,
        default=10000,
        help='Chunk size for processing'
    )
    
    parser.add_argument(
        '-w', '--workers',
        type=int,
        default=4,
        help='Number of parallel workers'
    )
    
    parser.add_argument(
        '--drop',
        action='store_true',
        help='Drop table if exists'
    )
    
    return parser.parse_args()

# Update main() function
def main():
    """Main entry point with CLI support"""
    
    args = parse_arguments()
    
    # Build configuration from arguments
    config = AppConfig()
    config.database.user = args.user
    config.database.password = args.password
    config.database.dsn = args.dsn
    config.loader.staging_table = args.table
    config.loader.chunk_size = args.chunk_size
    config.loader.max_workers = args.workers
    config.loader.drop_if_exists = args.drop
    config.file.input_file = args.file
    
    # Create and run application
    app = DataLoaderApp(config)
    success = app.run()
    
    sys.exit(0 if success else 1)Execute with Arguments:python -m enterprise_data_loader.main \
    --file data.csv \
    --table STG_CUSTOMER_DATA \
    --user myuser \
    --password mypass \
    --dsn localhost:1521/XEPDB1 \
    --chunk-size 20000 \
    --workers 8 \
    --drop6. Batch Processing ScriptCreate batch_load.py for multiple files:"""
Batch load multiple files
"""

from pathlib import Path
from enterprise_data_loader.config import AppConfig
from enterprise_data_loader.main import DataLoaderApp

def batch_load_files(file_list, base_config):
    """Load multiple files sequentially"""
    
    results = []
    
    for file_path, table_name in file_list:
        print(f"
{'='*70}")
        print(f"Loading {file_path} -> {table_name}")
        print(f"{'='*70}")
        
        # Clone config and update
        config = AppConfig.from_dict(base_config.__dict__)
        config.file.input_file = file_path
        config.loader.staging_table = table_name
        
        # Run loader
        app = DataLoaderApp(config)
        success = app.run()
        
        results.append({
            'file': file_path,
            'table': table_name,
            'success': success
        })
    
    # Print summary
    print(f"
{'='*70}")
    print("BATCH LOAD SUMMARY")
    print(f"{'='*70}")
    for result in results:
        status = "✓ SUCCESS" if result['success'] else "✗ FAILED"
        print(f"{result['file']:30} -> {result['table']:20} {status}")

if __name__ == "__main__":
    # Base configuration
    config = AppConfig()
    config.database.user = 'myuser'
    config.database.password = 'mypass'
    config.database.dsn = 'localhost:1521/XEPDB1'
    
    # Files to load
    files = [
        ('customers.csv', 'STG_CUSTOMERS'),
        ('orders.csv', 'STG_ORDERS'),
        ('products.xml', 'STG_PRODUCTS'),
    ]
    
    batch_load_files(files, config)7. TroubleshootingCheck Oracle Connection:# test_connection.py
import oracledb

try:
    conn = oracledb.connect(
        user='myuser',
        password='mypass',
        dsn='localhost:1521/XEPDB1'
    )
    print("✓ Connection successful!")
    print(f"Oracle version: {conn.version}")
    conn.close()
except Exception as e:
    print(f"✗ Connection failed: {e}")View Logs:# Logs are created in the logs/ directory
tail -f logs/data_loader_*.logThe modular structure gives you flexibility to run it as a simple package, with custom configurations, or as part of larger data pipeline orchestration .
