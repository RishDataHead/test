Great question! I'll add command-line argument support using Python's argparse module. You need to modify ONE FILE: main.py �.Modified enterprise_data_loader/main.pyReplace the entire file with this version that supports command-line arguments:"""
Enterprise Data Loader - Main Entry Point
Supports command-line arguments for flexible execution
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Dict, Any

from config import AppConfig
from database.connection_pool import OracleConnectionPool
from parsers.csv_parser import CSVParser
from parsers.xml_parser import XMLParser
from loaders.data_loader import DataLoader
from utils.logger import LoggerFactory


# Setup logging
LoggerFactory.setup_logging()
logger = LoggerFactory.get_logger(__name__)


class DataLoaderApp:
    """Main application orchestrator"""
    
    def __init__(self, config: AppConfig):
        """
        Initialize application
        
        Args:
            config: Application configuration
        """
        self.config = config
        self.pool = None
    
    def _initialize_connection_pool(self):
        """Initialize database connection pool"""
        logger.info("Initializing database connection pool")
        
        self.pool = OracleConnectionPool(
            user=self.config.database.user,
            password=self.config.database.password,
            dsn=self.config.database.dsn,
            min_conn=self.config.database.min_pool_conn,
            max_conn=self.config.database.max_pool_conn
        )
    
    def _get_parser(self):
        """
        Get appropriate parser based on file type
        
        Returns:
            Parser instance
        """
        file_path = self.config.file.input_file
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.csv':
            return CSVParser(
                file_path=file_path,
                chunk_size=self.config.loader.chunk_size,
                encoding=self.config.file.file_encoding,
                delimiter=self.config.file.csv_delimiter
            )
        elif file_ext == '.xml':
            return XMLParser(
                file_path=file_path,
                record_tag=self.config.file.xml_record_tag,
                chunk_size=self.config.loader.chunk_size,
                schema_file=self.config.file.xml_schema_file
            )
        else:
            raise ValueError(f"Unsupported file type: {file_ext}")
    
    def _print_statistics(self, stats: Dict[str, Any]):
        """Print loading statistics with atomic rollback information"""
        # Handle None or missing stats
        if stats is None:
            print("
" + "="*70)
            print("ERROR: No statistics available")
            print("="*70 + "
")
            return
        
        print("
" + "="*70)
        print(" "*25 + "LOAD STATISTICS")
        print("="*70)
        
        if stats.get('atomic_mode', False):
            print(f"{'Load Mode:':<30} {'ATOMIC (All-or-Nothing)':>20}")
        else:
            print(f"{'Load Mode:':<30} {'BEST EFFORT':>20}")
        
        print(f"{'Total Rows Loaded:':<30} {stats.get('total_rows', 0):>20,}")
        print(f"{'Failed Rows:':<30} {stats.get('failed_rows', 0):>20,}")
        print(f"{'Total Processed:':<30} {stats.get('total_processed', 0):>20,}")
        print(f"{'Total Chunks:':<30} {stats.get('total_chunks', 0):>20}")
        print(f"{'Elapsed Time:':<30} {stats.get('elapsed_seconds', 0):>17.2f} sec")
        print(f"{'Throughput:':<30} {stats.get('rows_per_second', 0):>15,.0f} rows/sec")
        
        if stats.get('atomic_rollback', False):
            print("="*70)
            print(" "*20 + "⚠️  ATOMIC ROLLBACK EXECUTED  ⚠️")
            print("="*70)
            print(f"{'Rows Before Rollback:':<30} {stats.get('rows_before_rollback', 0):>20,}")
            print(f"{'Rows After Rollback:':<30} {0:>20,}")
            
            if stats.get('first_failure'):
                print(f"{'First Failed Chunk:':<30} {stats['first_failure'].get('chunk_id', 'Unknown'):>20}")
                print(f"{'Failure Reason:':<30}")
                print(f"  {stats['first_failure'].get('error', 'Unknown error')}")
            
            print("="*70)
            print(f"{'Final Status:':<30} {'❌ ALL DATA TRUNCATED':>20}")
        else:
            status = '✅ SUCCESS' if stats.get('success', False) else '❌ FAILED'
            print(f"{'Status:':<30} {status:>20}")
        
        print("="*70)
        
        if stats.get('failed_chunks'):
            print("
" + "="*70)
            print(" "*22 + "FAILED CHUNKS DETAILS")
            print("="*70)
            print(f"{'Chunk ID':<12} {'Rows Failed':<15} {'Error Message':<43}")
            print("-"*70)
            
            for fc in stats.get('failed_chunks', []):
                chunk_id = fc.get('chunk_id', 'Unknown')
                rows_failed = fc.get('rows_failed', 0)
                error_msg = fc.get('error_message', 'Unknown error')
                error_msg = error_msg[:40] + '...' if len(error_msg) > 40 else error_msg
                print(f"{chunk_id:<12} {rows_failed:<15} {error_msg:<43}")
            
            print("="*70)
            print(f"
Failed chunk IDs: {stats.get('failed_chunk_ids', [])}")
            print(f"Failed data saved to: ./failed_data/")
            
            if stats.get('summary_file'):
                print(f"Summary file: {stats.get('summary_file')}")
            
            if stats.get('atomic_rollback', False):
                print("
⚠️  NOTE: Table was TRUNCATED - NO DATA was loaded")
        
        # Show error message if present
        if stats.get('error'):
            print("
" + "="*70)
            print("ERROR DETAILS")
            print("="*70)
            print(f"{stats.get('error')}")
            print("="*70)
        
        print()
    
    def run(self) -> bool:
        """
        Execute data loading process
        
        Returns:
            True if successful
        """
        stats = None
        
        try:
            logger.info("Starting Enterprise Data Loader")
            logger.info(f"Input file: {self.config.file.input_file}")
            
            # Validate file exists
            if not Path(self.config.file.input_file).exists():
                logger.error(f"File not found: {self.config.file.input_file}")
                stats = {
                    'success': False,
                    'error': f"File not found: {self.config.file.input_file}",
                    'total_rows': 0,
                    'failed_rows': 0
                }
                self._print_statistics(stats)
                return False
            
            # Initialize connection pool
            self._initialize_connection_pool()
            
            # Get parser
            parser = self._get_parser()
            
            # Validate file
            if self.config.file.validate_schema:
                if not parser.validate():
                    logger.error("File validation failed")
                    stats = {
                        'success': False,
                        'error': 'File validation failed',
                        'total_rows': 0,
                        'failed_rows': 0
                    }
                    self._print_statistics(stats)
                    return False
            
            # Initialize loader
            loader = DataLoader(
                pool=self.pool,
                table_name=self.config.loader.staging_table,
                chunk_size=self.config.loader.chunk_size,
                max_workers=self.config.loader.max_workers,
                batch_errors=self.config.loader.batch_errors,
                atomic_load=True
            )
            
            # Load data
            stats = loader.load_from_generator(
                data_generator=parser.parse(),
                create_table=self.config.loader.create_table,
                drop_if_exists=self.config.loader.drop_if_exists
            )
            
            # Print results
            self._print_statistics(stats)
            
            return stats.get('success', False) if stats else False
            
        except Exception as e:
            logger.error(f"Application failed: {str(e)}", exc_info=True)
            
            if stats is None:
                stats = {
                    'success': False,
                    'error': str(e),
                    'total_rows': 0,
                    'failed_rows': 0
                }
            
            self._print_statistics(stats)
            return False
        
        finally:
            if self.pool:
                self.pool.close()
            logger.info("Application finished")


def parse_arguments():
    """
    Parse command-line arguments
    
    Returns:
        Parsed arguments namespace
    """
    parser = argparse.ArgumentParser(
        description='Enterprise Data Loader - Load CSV/XML files into Oracle',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Load CSV with defaults
  python main.py --file data.csv --table STG_EMPLOYEES
  
  # Pipe-delimited file
  python main.py --file data.psv --table STG_DATA --delimiter '|'
  
  # XML with schema validation
  python main.py --file data.xml --table STG_XML --xml-tag employee --schema schema.xsd
  
  # Custom chunk size and encoding
  python main.py --file data.csv --table STG_DATA --chunk-size 50000 --encoding latin-1
  
  # Use environment variables for credentials
  export ORACLE_USER=scott
  export ORACLE_PASSWORD=tiger
  python main.py --file data.csv --table STG_DATA --dsn localhost:1521/XEPDB1
        """
    )
    
    # Required arguments
    parser.add_argument(
        '--file', '-f',
        required=True,
        help='Path to input file (CSV or XML)'
    )
    
    parser.add_argument(
        '--table', '-t',
        required=True,
        help='Target Oracle table name'
    )
    
    # Database arguments
    db_group = parser.add_argument_group('Database Options')
    db_group.add_argument(
        '--user', '-u',
        default=os.getenv('ORACLE_USER', 'your_username'),
        help='Oracle username (default: $ORACLE_USER or "your_username")'
    )
    
    db_group.add_argument(
        '--password', '-p',
        default=os.getenv('ORACLE_PASSWORD', 'your_password'),
        help='Oracle password (default: $ORACLE_PASSWORD or "your_password")'
    )
    
    db_group.add_argument(
        '--dsn',
        default=os.getenv('ORACLE_DSN', 'localhost:1521/XEPDB1'),
        help='Oracle DSN (default: $ORACLE_DSN or "localhost:1521/XEPDB1")'
    )
    
    # File format arguments
    file_group = parser.add_argument_group('File Format Options')
    file_group.add_argument(
        '--delimiter', '-d',
        default=',',
        help='CSV delimiter (default: ",")'
    )
    
    file_group.add_argument(
        '--encoding', '-e',
        default='utf-8',
        help='File encoding (default: utf-8)'
    )
    
    file_group.add_argument(
        '--xml-tag',
        default='record',
        help='XML record tag name (default: record)'
    )
    
    file_group.add_argument(
        '--schema',
        help='XML schema file (XSD) for validation'
    )
    
    # Loader arguments
    loader_group = parser.add_argument_group('Loader Options')
    loader_group.add_argument(
        '--chunk-size',
        type=int,
        default=10000,
        help='Rows per chunk (default: 10000)'
    )
    
    loader_group.add_argument(
        '--batch-errors',
        action='store_true',
        help='Continue loading on row-level errors'
    )
    
    loader_group.add_argument(
        '--no-create-table',
        action='store_true',
        help='Do not auto-create table'
    )
    
    loader_group.add_argument(
        '--no-drop',
        action='store_true',
        help='Do not drop existing table before creation'
    )
    
    loader_group.add_argument(
        '--no-validate',
        action='store_true',
        help='Skip file validation'
    )
    
    return parser.parse_args()


def main():
    """Main entry point with command-line argument support"""
    
    # Parse command-line arguments
    args = parse_arguments()
    
    # Create configuration from arguments
    config = AppConfig()
    
    # Database configuration
    config.database.user = args.user
    config.database.password = args.password
    config.database.dsn = args.dsn
    
    # File configuration
    config.file.input_file = args.file
    config.file.file_encoding = args.encoding
    config.file.csv_delimiter = args.delimiter
    config.file.xml_record_tag = args.xml_tag
    config.file.xml_schema_file = args.schema
    config.file.validate_schema = not args.no_validate
    
    # Loader configuration
    config.loader.staging_table = args.table
    config.loader.chunk_size = args.chunk_size
    config.loader.batch_errors = args.batch_errors
    config.loader.create_table = not args.no_create_table
    config.loader.drop_if_exists = not args.no_drop
    
    # Log configuration
    logger.info("="*70)
    logger.info("CONFIGURATION")
    logger.info("="*70)
    logger.info(f"File: {config.file.input_file}")
    logger.info(f"Table: {config.loader.staging_table}")
    logger.info(f"DSN: {config.database.dsn}")
    logger.info(f"User: {config.database.user}")
    logger.info(f"Chunk Size: {config.loader.chunk_size:,}")
    logger.info(f"Delimiter: '{config.file.csv_delimiter}'")
    logger.info(f"Encoding: {config.file.file_encoding}")
    logger.info("="*70)
    
    # Create and run application
    app = DataLoaderApp(config)
    success = app.run()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()Usage Examples1. Basic CSV Loadpython main.py --file employees.csv --table STG_EMPLOYEES2. With Database Credentialspython main.py \
  --file data.csv \
  --table STG_DATA \
  --user scott \
  --password tiger \
  --dsn localhost:1521/XEPDB13. Pipe-Delimited Filepython main.py \
  --file data.psv \
  --table STG_DATA \
  --delimiter '|'4. Tab-Delimited with Encodingpython main.py \
  --file data.tsv \
  --table STG_DATA \
  --delimiter $'\t' \
  --encoding latin-15. XML with Schema Validationpython main.py \
  --file employees.xml \
  --table STG_EMPLOYEES \
  --xml-tag employee \
  --schema schema.xsd6. Large Chunks with Batch Errorspython main.py \
  --file bigdata.csv \
  --table STG_BIG \
  --chunk-size 50000 \
  --batch-errors7. Use Existing Tablepython main.py \
  --file data.csv \
  --table EXISTING_TABLE \
  --no-create-table8. Environment Variables for Credentialsexport ORACLE_USER=scott
export ORACLE_PASSWORD=tiger
export ORACLE_DSN=localhost:1521/XEPDB1

python main.py --file data.csv --table STG_DATAHelp Commandpython main.py --helpOutput:usage: main.py [-h] --file FILE --table TABLE [--user USER] [--password PASSWORD]
               [--dsn DSN] [--delimiter DELIMITER] [--encoding ENCODING]
               [--xml-tag XML_TAG] [--schema SCHEMA] [--chunk-size CHUNK_SIZE]
               [--batch-errors] [--no-create-table] [--no-drop] [--no-validate]

Enterprise Data Loader - Load CSV/XML files into Oracle

optional arguments:
  -h, --help            show this help message and exit
  --file FILE, -f FILE  Path to input file (CSV or XML)
  --table TABLE, -t TABLE
                        Target Oracle table name

Database Options:
  --user USER, -u USER  Oracle username (default: $ORACLE_USER or "your_username")
  --password PASSWORD, -p PASSWORD
                        Oracle password (default: $ORACLE_PASSWORD or "your_password")
  --dsn DSN             Oracle DSN (default: $ORACLE_DSN or "localhost:1521/XEPDB1")

File Format Options:
  --delimiter DELIMITER, -d DELIMITER
                        CSV delimiter (default: ",")
  --encoding ENCODING, -e ENCODING
                        File encoding (default: utf-8)
  --xml-tag XML_TAG     XML record tag name (default: record)
  --schema SCHEMA       XML schema file (XSD) for validation

Loader Options:
  --chunk-size CHUNK_SIZE
                        Rows per chunk (default: 10000)
  --batch-errors        Continue loading on row-level errors
  --no-create-table     Do not auto-create table
  --no-drop             Do not drop existing table before creation
  --no-validate         Skip file validation

Examples:
  # Load CSV with defaults
  python main.py --file data.csv --table STG_EMPLOYEES
  
  # Pipe-delimited file
  python main.py --file data.psv --table STG_DATA --delimiter '|'
  
  # XML with schema validation
  python main.py --file data.xml --table STG_XML --xml-tag employee --schema schema.xsdShell Script WrapperCreate run_loader.sh for easier execution:#!/bin/bash

# Enterprise Data Loader Wrapper Script

# Default values
ORACLE_USER="${ORACLE_USER:-scott}"
ORACLE_PASSWORD="${ORACLE_PASSWORD:-tiger}"
ORACLE_DSN="${ORACLE_DSN:-localhost:1521/XEPDB1}"
CHUNK_SIZE="${CHUNK_SIZE:-10000}"

# Run loader
python main.py \
  --file "$1" \
  --table "$2" \
  --user "$ORACLE_USER" \
  --password "$ORACLE_PASSWORD" \
  --dsn "$ORACLE_DSN" \
  --chunk-size "$CHUNK_SIZE" \
  "${@:3}"

# Usage:
# ./run_loader.sh data.csv STG_DATA
# ./run_loader.sh data.psv STG_DATA --delimiter '|'Make it executable:chmod +x run_loader.shBenefits✅ Flexible: All parameters configurable via command line
✅ Secure: Credentials from environment variables
✅ CI/CD Ready: Perfect for automation scripts
✅ Self-documenting: Built-in help with examples
✅ Unix-friendly: Proper exit codes (0=success, 1=failure)Now your data loader is fully command-line driven and production-ready �!
