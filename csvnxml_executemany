"""
Enterprise Data Loader - CSV/XML to Oracle Database
Supports multithreaded loading with schema validation
Single file, ready to use
"""

import oracledb
import pandas as pd
import xml.etree.ElementTree as ET
from lxml import etree
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import time
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import os
import sys
from datetime import datetime


# ============================================================================
# CONFIGURATION SECTION - MODIFY THESE VALUES
# ============================================================================

CONFIG = {
    # Database Configuration
    'DB_USER': 'your_username',
    'DB_PASSWORD': 'your_password',
    'DB_DSN': 'localhost:1521/XEPDB1',  # host:port/service_name
    
    # Table Configuration
    'STAGING_TABLE': 'STG_DATA_LOAD',
    
    # Performance Configuration
    'CHUNK_SIZE': 10000,      # Rows per chunk
    'MAX_WORKERS': 4,          # Parallel threads
    'MIN_POOL_CONN': 2,
    'MAX_POOL_CONN': 10,
    
    # File Configuration
    'INPUT_FILE': 'data.csv',  # Can be .csv or .xml
    'FILE_ENCODING': 'utf-8',
    
    # XML Configuration (if using XML files)
    'XML_RECORD_TAG': 'record',  # Tag name for each record
    'XML_SCHEMA_FILE': None,      # Path to XSD schema file (optional)
    
    # Options
    'CREATE_TABLE': True,
    'DROP_IF_EXISTS': True,
    'BATCH_ERRORS': False,
    'VALIDATE_SCHEMA': True,
}


# ============================================================================
# LOGGING SETUP
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)-10s - %(levelname)-8s - %(message)s',
    handlers=[
        logging.FileHandler(f'data_loader_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONNECTION POOL
# ============================================================================

class OracleConnectionPool:
    """Thread-safe Oracle connection pool"""
    
    def __init__(self, user: str, password: str, dsn: str, 
                 min_conn: int = 2, max_conn: int = 10):
        try:
            self.pool = oracledb.create_pool(
                user=user,
                password=password,
                dsn=dsn,
                min=min_conn,
                max=max_conn,
                increment=1,
                threaded=True,
                getmode=oracledb.POOL_GETMODE_WAIT
            )
            logger.info(f"Connection pool created: min={min_conn}, max={max_conn}")
        except Exception as e:
            logger.error(f"Failed to create connection pool: {str(e)}")
            raise
    
    @contextmanager
    def get_connection(self):
        conn = self.pool.acquire()
        try:
            yield conn
        finally:
            self.pool.release(conn)
    
    def close(self):
        self.pool.close()
        logger.info("Connection pool closed")


# ============================================================================
# XML PARSER WITH SCHEMA VALIDATION
# ============================================================================

class XMLParser:
    """Parse and validate XML files"""
    
    def __init__(self, xml_file: str, record_tag: str, schema_file: Optional[str] = None):
        self.xml_file = xml_file
        self.record_tag = record_tag
        self.schema_file = schema_file
        
    def validate_schema(self) -> bool:
        """Validate XML against XSD schema"""
        if not self.schema_file:
            logger.warning("No schema file provided, skipping validation")
            return True
            
        try:
            logger.info(f"Validating XML against schema: {self.schema_file}")
            with open(self.schema_file, 'r') as f:
                schema_root = etree.XML(f.read())
            schema = etree.XMLSchema(schema_root)
            
            xml_doc = etree.parse(self.xml_file)
            
            if schema.validate(xml_doc):
                logger.info("XML schema validation: PASSED")
                return True
            else:
                logger.error(f"XML schema validation: FAILED")
                for error in schema.error_log:
                    logger.error(f"  Line {error.line}: {error.message}")
                return False
                
        except Exception as e:
            logger.error(f"Schema validation error: {str(e)}")
            return False
    
    def parse_to_dataframe(self, chunk_size: int = 10000):
        """
        Parse XML file and yield DataFrames in chunks
        Generator function for memory efficiency
        """
        try:
            logger.info(f"Parsing XML file: {self.xml_file}")
            
            # Parse XML iteratively for large files
            context = ET.iterparse(self.xml_file, events=('end',))
            
            records = []
            count = 0
            
            for event, elem in context:
                if elem.tag == self.record_tag:
                    # Extract all child elements as dictionary
                    record = {}
                    for child in elem:
                        record[child.tag] = child.text
                    
                    records.append(record)
                    count += 1
                    
                    # Clear element to free memory
                    elem.clear()
                    
                    # Yield chunk when size reached
                    if len(records) >= chunk_size:
                        df = pd.DataFrame(records)
                        yield df
                        records = []
            
            # Yield remaining records
            if records:
                df = pd.DataFrame(records)
                yield df
            
            logger.info(f"XML parsing complete: {count} records processed")
            
        except Exception as e:
            logger.error(f"XML parsing error: {str(e)}")
            raise


# ============================================================================
# DATA LOADER
# ============================================================================

class DataLoader:
    """Enterprise data loader with multithreading support"""
    
    def __init__(self, pool: OracleConnectionPool, table_name: str,
                 chunk_size: int = 10000, max_workers: int = 4,
                 batch_errors: bool = False):
        self.pool = pool
        self.table_name = table_name
        self.chunk_size = chunk_size
        self.max_workers = max_workers
        self.batch_errors = batch_errors
        self.stats_lock = Lock()
        self.total_rows = 0
        self.failed_rows = 0
    
    def _validate_dataframe(self, df: pd.DataFrame) -> bool:
        """Validate DataFrame structure and data quality"""
        try:
            # Check for empty DataFrame
            if df.empty:
                logger.warning("DataFrame is empty")
                return False
            
            # Check for duplicate columns
            if df.columns.duplicated().any():
                logger.error("DataFrame has duplicate column names")
                return False
            
            # Log data quality metrics
            null_counts = df.isnull().sum()
            if null_counts.any():
                logger.info("Null value counts:")
                for col, count in null_counts[null_counts > 0].items():
                    logger.info(f"  {col}: {count} nulls ({count/len(df)*100:.2f}%)")
            
            return True
            
        except Exception as e:
            logger.error(f"DataFrame validation error: {str(e)}")
            return False
    
    def _prepare_insert_statement(self, columns: List[str]) -> str:
        """Generate parameterized INSERT statement"""
        cols = ', '.join(columns)
        placeholders = ', '.join([f':{i+1}' for i in range(len(columns))])
        return f"INSERT INTO {self.table_name} ({cols}) VALUES ({placeholders})"
    
    def _load_chunk(self, chunk_data: pd.DataFrame, chunk_id: int) -> Dict[str, Any]:
        """Load a single chunk into Oracle"""
        start_time = time.time()
        rows_inserted = 0
        
        try:
            with self.pool.get_connection() as conn:
                cursor = conn.cursor()
                cursor.setinputsizes(None, self.chunk_size)
                
                columns = chunk_data.columns.tolist()
                insert_sql = self._prepare_insert_statement(columns)
                
                # Handle NaN/None values
                chunk_data = chunk_data.where(pd.notnull(chunk_data), None)
                data_tuples = [tuple(row) for row in chunk_data.values]
                
                # Batch insert
                if self.batch_errors:
                    cursor.executemany(insert_sql, data_tuples, batcherrors=True)
                    errors = cursor.getbatcherrors()
                    if errors:
                        logger.warning(f"Chunk {chunk_id}: {len(errors)} row errors")
                        rows_inserted = len(data_tuples) - len(errors)
                    else:
                        rows_inserted = len(data_tuples)
                else:
                    cursor.executemany(insert_sql, data_tuples)
                    rows_inserted = len(data_tuples)
                
                conn.commit()
                cursor.close()
                
            elapsed = time.time() - start_time
            throughput = rows_inserted / elapsed if elapsed > 0 else 0
            
            logger.info(
                f"Chunk {chunk_id}: {rows_inserted} rows in {elapsed:.2f}s "
                f"({throughput:.0f} rows/sec)"
            )
            
            return {
                'chunk_id': chunk_id,
                'rows_inserted': rows_inserted,
                'success': True,
                'elapsed': elapsed
            }
            
        except Exception as e:
            logger.error(f"Chunk {chunk_id} failed: {str(e)}")
            return {
                'chunk_id': chunk_id,
                'rows_inserted': 0,
                'success': False,
                'error': str(e)
            }
    
    def _create_staging_table(self, sample_df: pd.DataFrame, drop_if_exists: bool = True):
        """Create staging table from DataFrame schema"""
        try:
            with self.pool.get_connection() as conn:
                cursor = conn.cursor()
                
                if drop_if_exists:
                    try:
                        cursor.execute(f"DROP TABLE {self.table_name} PURGE")
                        logger.info(f"Dropped existing table: {self.table_name}")
                    except oracledb.DatabaseError:
                        pass
                
                type_mapping = {
                    'int64': 'NUMBER',
                    'int32': 'NUMBER',
                    'float64': 'NUMBER',
                    'float32': 'NUMBER',
                    'object': 'VARCHAR2(4000)',
                    'bool': 'NUMBER(1)',
                    'datetime64[ns]': 'TIMESTAMP',
                    'datetime64': 'TIMESTAMP'
                }
                
                columns_def = []
                for col, dtype in sample_df.dtypes.items():
                    clean_col = col.replace(' ', '_').replace('-', '_').upper()
                    oracle_type = type_mapping.get(str(dtype), 'VARCHAR2(4000)')
                    columns_def.append(f"{clean_col} {oracle_type}")
                
                create_sql = f"CREATE TABLE {self.table_name} ({', '.join(columns_def)})"
                cursor.execute(create_sql)
                conn.commit()
                
                logger.info(f"Created table: {self.table_name}")
                logger.info(f"Columns: {', '.join([cd.split()[0] for cd in columns_def])}")
                cursor.close()
                
        except Exception as e:
            logger.error(f"Failed to create table: {str(e)}")
            raise
    
    def load_from_generator(self, data_generator, create_table: bool = True,
                           drop_if_exists: bool = True) -> Dict[str, Any]:
        """Load data from a generator that yields DataFrames"""
        overall_start = time.time()
        
        # Get first chunk
        try:
            first_chunk = next(data_generator)
        except StopIteration:
            logger.error("No data to load")
            return {'success': False, 'error': 'No data found'}
        
        # Validate first chunk
        if not self._validate_dataframe(first_chunk):
            logger.error("Data validation failed")
            return {'success': False, 'error': 'Validation failed'}
        
        # Create table if needed
        if create_table:
            self._create_staging_table(first_chunk, drop_if_exists)
        
        # Process chunks with multithreading
        chunk_id = 0
        futures = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit first chunk
            futures.append(executor.submit(self._load_chunk, first_chunk, chunk_id))
            chunk_id += 1
            
            # Submit remaining chunks
            for chunk in data_generator:
                futures.append(executor.submit(self._load_chunk, chunk, chunk_id))
                chunk_id += 1
            
            # Collect results
            for future in as_completed(futures):
                result = future.result()
                with self.stats_lock:
                    if result['success']:
                        self.total_rows += result['rows_inserted']
                    else:
                        self.failed_rows += result.get('rows_inserted', 0)
        
        overall_elapsed = time.time() - overall_start
        
        return {
            'total_rows': self.total_rows,
            'failed_rows': self.failed_rows,
            'total_chunks': chunk_id,
            'elapsed_seconds': overall_elapsed,
            'rows_per_second': self.total_rows / overall_elapsed if overall_elapsed > 0 else 0,
            'success': self.failed_rows == 0
        }


# ============================================================================
# FILE PROCESSORS
# ============================================================================

def process_csv(file_path: str, chunk_size: int, encoding: str):
    """Generator for CSV chunks"""
    logger.info(f"Processing CSV file: {file_path}")
    return pd.read_csv(file_path, chunksize=chunk_size, 
                      low_memory=False, encoding=encoding)


def process_xml(file_path: str, chunk_size: int, record_tag: str,
                schema_file: Optional[str], validate: bool):
    """Generator for XML chunks"""
    logger.info(f"Processing XML file: {file_path}")
    
    parser = XMLParser(file_path, record_tag, schema_file)
    
    # Validate schema if required
    if validate and schema_file:
        if not parser.validate_schema():
            raise ValueError("XML schema validation failed")
    
    return parser.parse_to_dataframe(chunk_size)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def print_statistics(stats: Dict[str, Any]):
    """Print loading statistics"""
    print("\n" + "="*70)
    print(" "*25 + "LOAD STATISTICS")
    print("="*70)
    print(f"{'Total Rows Loaded:':<30} {stats['total_rows']:>20,}")
    print(f"{'Failed Rows:':<30} {stats['failed_rows']:>20,}")
    print(f"{'Total Chunks:':<30} {stats['total_chunks']:>20}")
    print(f"{'Elapsed Time:':<30} {stats['elapsed_seconds']:>17.2f} sec")
    print(f"{'Throughput:':<30} {stats['rows_per_second']:>15,.0f} rows/sec")
    print(f"{'Status:':<30} {('SUCCESS' if stats['success'] else 'FAILED'):>20}")
    print("="*70 + "\n")


def main():
    """Main execution function"""
    
    logger.info("Starting Data Loader")
    logger.info(f"Configuration: {CONFIG['INPUT_FILE']}")
    
    # Override config with environment variables if present
    db_user = os.getenv('ORACLE_USER', CONFIG['DB_USER'])
    db_password = os.getenv('ORACLE_PASSWORD', CONFIG['DB_PASSWORD'])
    db_dsn = os.getenv('ORACLE_DSN', CONFIG['DB_DSN'])
    
    # Determine file type
    file_ext = os.path.splitext(CONFIG['INPUT_FILE'])[1].lower()
    
    if not os.path.exists(CONFIG['INPUT_FILE']):
        logger.error(f"File not found: {CONFIG['INPUT_FILE']}")
        return
    
    # Create connection pool
    pool = OracleConnectionPool(
        user=db_user,
        password=db_password,
        dsn=db_dsn,
        min_conn=CONFIG['MIN_POOL_CONN'],
        max_conn=CONFIG['MAX_POOL_CONN']
    )
    
    try:
        # Initialize loader
        loader = DataLoader(
            pool=pool,
            table_name=CONFIG['STAGING_TABLE'],
            chunk_size=CONFIG['CHUNK_SIZE'],
            max_workers=CONFIG['MAX_WORKERS'],
            batch_errors=CONFIG['BATCH_ERRORS']
        )
        
        # Process file based on type
        if file_ext == '.csv':
            data_gen = process_csv(
                CONFIG['INPUT_FILE'],
                CONFIG['CHUNK_SIZE'],
                CONFIG['FILE_ENCODING']
            )
        elif file_ext == '.xml':
            data_gen = process_xml(
                CONFIG['INPUT_FILE'],
                CONFIG['CHUNK_SIZE'],
                CONFIG['XML_RECORD_TAG'],
                CONFIG['XML_SCHEMA_FILE'],
                CONFIG['VALIDATE_SCHEMA']
            )
        else:
            logger.error(f"Unsupported file type: {file_ext}")
            return
        
        # Load data
        stats = loader.load_from_generator(
            data_gen,
            create_table=CONFIG['CREATE_TABLE'],
            drop_if_exists=CONFIG['DROP_IF_EXISTS']
        )
        
        # Print results
        print_statistics(stats)
        
    except Exception as e:
        logger.error(f"Load failed: {str(e)}", exc_info=True)
        raise
    finally:
        pool.close()
        logger.info("Data Loader finished")


if __name__ == "__main__":
    main()
