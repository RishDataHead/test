i don't want to use __main__ function.
i only want return from extract data to csv function,  is it possible ?
i want to use this script in another script

Yes, absolutely! You can make it a reusable function that returns results. Here's how to modify it:

***

## **Changes to make the script reusable:**

### **1. Modify the logging setup to be optional:**

**At the top (replace existing logging section):**

```python
import oracledb
import csv
import logging
from datetime import datetime
from multiprocessing import Process, Queue, cpu_count
import time

# Configuration
FETCH_BATCH_SIZE = 50000
WRITE_BATCH_SIZE = 100000
BUFFER_SIZE = 10 * 1024 * 1024

def setup_logging_if_needed(log_file=None):
    """Setup logging only if not already configured"""
    if not logging.getLogger().hasHandlers():
        if log_file is None:
            log_file = f'oracle_extract_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        
        logging.basicConfig(
            filename=log_file,
            level=logging.INFO,
            format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s',
            force=True
        )
        return log_file
    return None
```

***

### **2. Modify `extract_data_to_csv` to accept parameters and return results:**

**Replace the function signature:**

```python
def extract_data_to_csv(username, password, dsn, table_name, output_file=None, log_file=None):
    """
    Extract data from Oracle Database to CSV file.
    
    Args:
        username: Oracle username
        password: Oracle password
        dsn: Oracle DSN (e.g., 'host:1521/service_name')
        table_name: Name of table to extract
        output_file: Output CSV filename (default: 'oracle_table_data.csv')
        log_file: Log filename (default: auto-generated with timestamp)
    
    Returns:
        dict: {
            'success': bool,
            'output_file': str,
            'log_file': str,
            'total_rows': int,
            'time_taken': float,
            'throughput': float,
            'error': str (if success=False)
        }
    """
    # Setup logging
    log_file = setup_logging_if_needed(log_file)
    
    # Set default output file
    if output_file is None:
        output_file = 'oracle_table_data.csv'
    
    conn = None
    cursor = None
    start_time = time.time()
    
    try:
        # Connect to Oracle Database
        conn = oracledb.connect(user=username, password=password, dsn=dsn)
        
        logging.info('Successfully connected to Oracle Database')
        print('✓ Connected to Oracle Database')
        
        cursor = conn.cursor()
        
        # Optimize fetch performance
        cursor.arraysize = FETCH_BATCH_SIZE
        cursor.prefetchrows = FETCH_BATCH_SIZE
        
        # Execute query
        query = f'SELECT * FROM {table_name}'
        cursor.execute(query)
        
        logging.info(f'Query executed: {query}')
        print('✓ Query executed')
        
        # Get column names
        column_names = [col[0] for col in cursor.description]
        
        # Write header to CSV file
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(column_names)
        
        logging.info('CSV header written')
        print('✓ CSV file created with headers')
        
        # Create queue
        write_queue = Queue(maxsize=100)
        
        # Start writer processes
        num_writers = min(4, cpu_count() - 1 or 1)
        writers = []
        
        for i in range(num_writers):
            p = Process(target=csv_writer_process, args=(write_queue, i, output_file))
            p.start()
            writers.append(p)
            logging.info(f'Started CSV writer process {i}')
        
        print(f'✓ Started {num_writers} writer processes')
        
        # Fetch data and distribute to writers
        row_count = 0
        batch = []
        fetch_count = 0
        
        print('\nExtracting data...')
        
        while True:
            rows = cursor.fetchmany(FETCH_BATCH_SIZE)
            if not rows:
                break
            
            fetch_count += 1
            
            for row in rows:
                batch.append(row)
                row_count += 1
                
                if len(batch) >= WRITE_BATCH_SIZE:
                    write_queue.put(batch)
                    batch = []
            
            if fetch_count % 10 == 0:
                print(f'  Processed {row_count:,} rows...')
                logging.info(f'Processed {row_count} rows')
        
        # Send remaining rows
        if batch:
            write_queue.put(batch)
        
        # Send poison pills
        for _ in range(num_writers):
            write_queue.put(None)
        
        # Wait for all writers
        for p in writers:
            p.join()
        
        elapsed_time = time.time() - start_time
        rows_per_second = row_count / elapsed_time if elapsed_time > 0 else 0
        
        logging.info(f'Extraction completed successfully')
        logging.info(f'Total rows: {row_count}')
        logging.info(f'Time taken: {elapsed_time:.2f} seconds')
        logging.info(f'Throughput: {rows_per_second:.2f} rows/second')
        
        print(f'\n{"="*60}')
        print(f'✓ Data extraction SUCCESSFUL!')
        print(f'{"="*60}')
        print(f'  Output file    : {output_file}')
        print(f'  Total rows     : {row_count:,}')
        print(f'  Time taken     : {elapsed_time:.2f} seconds')
        print(f'  Throughput     : {rows_per_second:,.0f} rows/second')
        if log_file:
            print(f'  Log file       : {log_file}')
        print(f'{"="*60}')
        
        return {
            'success': True,
            'output_file': output_file,
            'log_file': log_file,
            'total_rows': row_count,
            'time_taken': elapsed_time,
            'throughput': rows_per_second,
            'error': None
        }

    except Exception as e:
        error_msg = str(e)
        logging.error(f'Error: {error_msg}')
        print(f'\n✗ Error occurred!')
        print(f'  Error: {error_msg}')
        if log_file:
            print(f'  Check log file: {log_file}')
        
        return {
            'success': False,
            'output_file': output_file,
            'log_file': log_file,
            'total_rows': 0,
            'time_taken': time.time() - start_time,
            'throughput': 0,
            'error': error_msg
        }

    finally:
        if cursor:
            cursor.close()
            logging.info('Cursor closed')
        if conn:
            conn.close()
            logging.info('Oracle connection closed')
```

***

### **3. Update `csv_writer_process` to accept output_file:**

**Replace function signature:**

```python
def csv_writer_process(write_queue, process_id, output_file):
    """Dedicated writer process"""
    try:
        total_rows = 0
        
        while True:
            batch = write_queue.get()
            
            if batch is None:
                logging.info(f'Writer process {process_id} finished: {total_rows} rows written')
                break
            
            with open(output_file, 'a', newline='', encoding='utf-8', 
                     buffering=BUFFER_SIZE) as f:
                writer = csv.writer(f)
                writer.writerows(batch)
                total_rows += len(batch)
        
    except Exception as e:
        logging.error(f'Writer process {process_id} error: {str(e)}')
        raise
```

***

### **4. Optional: Add `if __name__ == '__main__'` for testing only:**

```python
if __name__ == '__main__':
    # Example usage when running directly
    result = extract_data_to_csv(
        username='your_username',
        password='your_password',
        dsn='your_host:1521/your_service_name',
        table_name='your_table_name',
        output_file='my_data.csv'
    )
    
    if result['success']:
        print(f"\nExtracted {result['total_rows']} rows successfully!")
    else:
        print(f"\nExtraction failed: {result['error']}")
```

***

### **5. Usage from another script:**

```python
# In your other script
from your_script_name import extract_data_to_csv

# Call the function
result = extract_data_to_csv(
    username='myuser',
    password='mypass',
    dsn='localhost:1521/XEPDB1',
    table_name='EMPLOYEES',
    output_file='employees_export.csv',
    log_file='my_custom_log.log'  # Optional
)

# Check result
if result['success']:
    print(f"Success! Extracted {result['total_rows']} rows")
    print(f"Output: {result['output_file']}")
else:
    print(f"Failed: {result['error']}")
```

***

This makes the script fully reusable as a module with proper return values and error handling[1][2].

Citations:
[1] selected_image_4695495282969847714.jpg https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/92825140/e6e369e6-8e33-4ec6-8311-3998c4ebe9de/selected_image_4695495282969847714.jpg
[2] selected_image_7109357792060058343.jpg https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/images/92825140/71d529ae-727e-4e69-8d2d-842123a20f5f/selected_image_7109357792060058343.jpg
